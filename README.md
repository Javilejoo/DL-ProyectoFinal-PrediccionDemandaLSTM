# ğŸ« PredicciÃ³n de Demanda de Chocolates con LSTM

## ğŸ“‹ DescripciÃ³n del Proyecto

Sistema de predicciÃ³n de demanda semanal de chocolates utilizando redes neuronales recurrentes (LSTM - Long Short-Term Memory). Este proyecto aplica tÃ©cnicas de Deep Learning para forecasting de series temporales, capturando patrones estacionales, tendencias y efectos de eventos especiales.

**Autores:**
- Javier Prado - 21486
- Bryan EspaÃ±a - 21550

**Curso:** CC3092 - Deep Learning y Sistemas Inteligentes  
**Semestre:** II - 2025

---

## ğŸ¯ Problema

Las empresas de retail necesitan predecir con precisiÃ³n la demanda de productos para:
- Optimizar niveles de inventario
- Reducir costos de almacenamiento
- Evitar quiebres de stock
- Mejorar la planificaciÃ³n de producciÃ³n
- Maximizar ventas en temporadas pico

**DesafÃ­o especÃ­fico:** Predecir la demanda semanal de chocolates considerando:
- Estacionalidad (San ValentÃ­n, Navidad, DÃ­a de la Madre, etc.)
- Promociones y descuentos
- Tendencias anuales
- Patrones histÃ³ricos

---

## ğŸ”¬ Propuesta de SoluciÃ³n

ImplementaciÃ³n de un modelo **LSTM (Long Short-Term Memory)** que:

1. **Procesa secuencias temporales** de 12 semanas histÃ³ricas
2. **Predice la demanda** para 1 semana adelante
3. **Incorpora mÃºltiples features:**
   - Componentes temporales (mes, semana del aÃ±o, progreso anual)
   - Lags de demanda (1, 4, 12, 52 semanas)
   - EstadÃ­sticas mÃ³viles (media y desviaciÃ³n de 8 semanas)
   - Indicadores de festividades y promociones

### Â¿Por quÃ© LSTM?

- âœ… Captura dependencias de largo plazo en series temporales
- âœ… Maneja efecto de "memoria" para patrones estacionales
- âœ… Robusta ante ruido y variabilidad en los datos
- âœ… Superior a modelos tradicionales (ARIMA, Exponential Smoothing) para series complejas

---

## ğŸ—ï¸ Arquitectura del Modelo

```
Input: (batch_size, 12 timesteps, 15 features)
    â†“
LSTM Layer 1: 128 units + Dropout (0.2)
    â†“
LSTM Layer 2: 64 units + Dropout (0.2)
    â†“
Dense Layer: 32 units (ReLU)
    â†“
Output Layer: 1 unit (Linear) â†’ PredicciÃ³n de demanda (log1p)
```

**HiperparÃ¡metros:**
- Lookback: 12 semanas
- Optimizer: Adam (learning rate = 0.001)
- Loss: MSE (Mean Squared Error)
- Batch size: 16
- Early Stopping: patience = 15 epochs
- ReduceLROnPlateau: factor = 0.5, patience = 7

---

## ğŸ“Š Dataset

### Estructura de Datos

**PerÃ­odo:** Enero 2018 - Diciembre 2024 (366 semanas)

**Splits:**
- **Train:** 2018-2022 (260 semanas)
- **Test:** 2023 (52 semanas)
- **Validation:** 2024 (52 semanas)

### Features (15 variables)

#### 1. Componentes Temporales (6)
- `sin_woy`, `cos_woy`: CodificaciÃ³n cÃ­clica de semana del aÃ±o
- `sin_month`, `cos_month`: CodificaciÃ³n cÃ­clica de mes
- `year_progress`: Progreso dentro del aÃ±o (0-1)
- `weeks_from_start`: Semanas desde inicio del dataset

#### 2. Indicadores de Eventos (3)
- `holiday_flag`: Semana contiene festividad
- `holiday_lead_flag`: Semana previa a festividad
- `holiday_decay_flag`: Semana posterior a festividad

#### 3. Features HistÃ³ricas (6)
- `demand_lag_1`: Demanda hace 1 semana
- `demand_lag_4`: Demanda hace 4 semanas
- `demand_lag_12`: Demanda hace 12 semanas (3 meses)
- `demand_lag_52`: Demanda hace 52 semanas (1 aÃ±o)
- `demand_rolling_mean_8w`: Media mÃ³vil de 8 semanas
- `demand_rolling_std_8w`: DesviaciÃ³n estÃ¡ndar mÃ³vil de 8 semanas

### Target Variable
- `y_tr`: Demanda transformada con log1p â†’ `log(1 + demand)`
- `demand`: Demanda real en unidades

---

## ğŸš€ InstalaciÃ³n y Uso

### Requisitos

```bash
pip install numpy pandas matplotlib seaborn scikit-learn tensorflow jupyter
```

### Estructura del Proyecto

```
DL-ProyectoFinal-PrediccionDemandaLSTM/
â”‚
â”œâ”€â”€ data/                                    # Datasets
â”‚   â”œâ”€â”€ demand_weekly_chocolates_2018-2024.csv
â”‚   â”œâ”€â”€ demand_weekly_chocolates_train_features.csv
â”‚   â”œâ”€â”€ demand_weekly_chocolates_test_features.csv
â”‚   â””â”€â”€ demand_weekly_chocolates_valid_features.csv
â”‚
â”œâ”€â”€ models/                                  # Modelos entrenados
â”‚   â”œâ”€â”€ best_lstm_model.h5
â”‚   â””â”€â”€ scaler.pkl
â”‚
â”œâ”€â”€ results/                                 # Resultados
â”‚   â”œâ”€â”€ metrics_summary.json
â”‚   â”œâ”€â”€ predictions_train.csv
â”‚   â”œâ”€â”€ predictions_test.csv
â”‚   â””â”€â”€ predictions_validation.csv
â”‚
â”œâ”€â”€ plots/                                   # Visualizaciones
â”‚   â”œâ”€â”€ learning_curves.png
â”‚   â”œâ”€â”€ predictions_all_sets.png
â”‚   â””â”€â”€ residuals_analysis.png
â”‚
â”œâ”€â”€ dataset.py                               # Generador de datos sintÃ©ticos
â”œâ”€â”€ model_lstm.py                            # Clase del modelo LSTM
â”œâ”€â”€ predicciones.ipynb                       # Notebook principal
â””â”€â”€ README.md                                # Este archivo
```

### EjecuciÃ³n

#### OpciÃ³n 1: Usando el Notebook (Recomendado)

```bash
jupyter notebook predicciones.ipynb
```

Ejecuta las celdas en orden para:
1. Cargar y explorar datos
2. Crear features de ingenierÃ­a
3. Entrenar el modelo LSTM
4. Evaluar y visualizar resultados

#### OpciÃ³n 2: Usando el Script Python

```bash
python model_lstm.py
```

Este script entrena el modelo automÃ¡ticamente y guarda los resultados.

---

## ğŸ“ˆ Resultados

### MÃ©tricas de EvaluaciÃ³n

Las mÃ©tricas se reportan en **escala original** (unidades de demanda):

| Set | MAE | RMSE | RÂ² | MAPE |
|-----|-----|------|-----|------|
| **Train** | 13.90 u | 18.89 u | 0.1166 | 25.35% |
| **Validation** | 15.93 u | 20.00 u | 0.0091 | 27.65% |
| **Test** | 16.42 u | 20.92 u | 0.1415 | 33.83% |

**InterpretaciÃ³n:** Ver `ANALISIS_RESULTADOS.md` para anÃ¡lisis detallado de las mÃ©tricas.

### InterpretaciÃ³n de MÃ©tricas

- **MAE (Mean Absolute Error):** Error promedio en unidades. Ejemplo: MAE=5 â†’ el modelo se equivoca en promedio por 5 unidades.
- **RMSE (Root Mean Squared Error):** Penaliza errores grandes. Ãštil para detectar outliers.
- **RÂ² (Coeficiente de DeterminaciÃ³n):** Bondad de ajuste. Valores cercanos a 1 indican excelente ajuste.
- **MAPE (Mean Absolute Percentage Error):** Error porcentual. Ãštil para comparar con benchmarks.

### Visualizaciones Generadas

1. **learning_curves.png:** EvoluciÃ³n del loss y MAE durante entrenamiento
2. **predictions_all_sets.png:** ComparaciÃ³n de predicciones vs valores reales
3. **residuals_analysis.png:** AnÃ¡lisis de errores y distribuciÃ³n de residuos

---

## ğŸ› ï¸ Herramientas y TecnologÃ­as

### LibrerÃ­as Principales

- **TensorFlow/Keras:** Framework de Deep Learning para construir el modelo LSTM
- **NumPy:** Operaciones numÃ©ricas y manejo de arrays
- **Pandas:** ManipulaciÃ³n y anÃ¡lisis de datos
- **Scikit-learn:** Preprocesamiento (StandardScaler) y mÃ©tricas
- **Matplotlib/Seaborn:** Visualizaciones

### TÃ©cnicas de Deep Learning Aplicadas

1. **LSTM (Long Short-Term Memory)**
   - Redes neuronales recurrentes especializadas en secuencias
   - Cell state y gates (forget, input, output) para memoria selectiva
   
2. **Dropout Regularization**
   - Previene overfitting desactivando aleatoriamente neuronas
   
3. **Early Stopping**
   - Detiene entrenamiento cuando validation loss deja de mejorar
   
4. **Learning Rate Scheduling (ReduceLROnPlateau)**
   - Reduce automÃ¡ticamente el learning rate cuando se estanca
   
5. **Batch Normalization** (implÃ­cito en normalizaciÃ³n de features)
   - StandardScaler para estabilizar el entrenamiento

### TÃ©cnicas de Feature Engineering

- **CodificaciÃ³n cÃ­clica:** `sin/cos` para variables temporales periÃ³dicas
- **Lagged features:** Valores histÃ³ricos como predictores
- **Rolling statistics:** Captura tendencias locales
- **Log transformation:** Estabiliza varianza y normaliza distribuciÃ³n

---

## ğŸ” AnÃ¡lisis Exploratorio de Datos (EDA)

El notebook incluye:

- âœ… AnÃ¡lisis de tendencias temporales
- âœ… DetecciÃ³n de estacionalidad
- âœ… CorrelaciÃ³n entre features
- âœ… DistribuciÃ³n de demanda
- âœ… Impacto de promociones y festividades
- âœ… IdentificaciÃ³n de outliers

---

## ğŸ“ Conclusiones

1. **El modelo LSTM captura efectivamente patrones complejos** en series temporales de demanda, incluyendo:
   - Estacionalidad anual y mensual
   - Efectos de festividades (San ValentÃ­n, Navidad, etc.)
   - Impacto de promociones
   - Tendencias de largo plazo

2. **Las features de ingenierÃ­a son crÃ­ticas:**
   - Los lags de 52 semanas capturan estacionalidad anual
   - Las estadÃ­sticas mÃ³viles ayudan a suavizar predicciones
   - Los indicadores de festividades mejoran precisiÃ³n en fechas clave

3. **El modelo generaliza bien** en datos no vistos (test set), demostrando robustez.

4. **Aplicaciones prÃ¡cticas:**
   - OptimizaciÃ³n de inventario
   - PlanificaciÃ³n de producciÃ³n
   - Estrategias de pricing dinÃ¡mico
   - GestiÃ³n de cadena de suministro

---

## ğŸ“š Referencias BibliogrÃ¡ficas

1. Hochreiter, S., & Schmidhuber, J. (1997). "Long Short-Term Memory". Neural Computation, 9(8), 1735-1780.

2. Box, G. E., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). "Time Series Analysis: Forecasting and Control" (5th ed.). Wiley.

3. Chollet, F. (2021). "Deep Learning with Python" (2nd ed.). Manning Publications.

4. Hyndman, R. J., & Athanasopoulos, G. (2021). "Forecasting: Principles and Practice" (3rd ed.). OTexts.

5. TensorFlow Documentation: "Time Series Forecasting". https://www.tensorflow.org/tutorials/structured_data/time_series

6. Goodfellow, I., Bengio, Y., & Courville, A. (2016). "Deep Learning". MIT Press.

---

## ğŸ“ Licencia

Este proyecto fue desarrollado con fines acadÃ©micos para el curso CC3092 - Deep Learning y Sistemas Inteligentes de la Universidad del Valle de Guatemala.

---

## ğŸ“§ Contacto

Para preguntas o colaboraciones:
- **Javier Prado:** [21486@uvg.edu.gt](mailto:21486@uvg.edu.gt)
- **Bryan EspaÃ±a:** [21550@uvg.edu.gt](mailto:21550@uvg.edu.gt)

---

**Â¡Gracias por revisar nuestro proyecto! ğŸš€**